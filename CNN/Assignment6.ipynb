{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Difference between TRAINABLE and NON-TRAINABLE PARAMETERS:\n",
    "TRAINABLE PARAMETERS are the model parameters that are updated during the training process using backpropagation and \n",
    "gradient descent. These parameters include weights and biases in layers like fully connected, convolutional, and recurrent \n",
    "layers.\n",
    "NON-TRAINABLE PARAMETERS are fixed and not updated during training. They often include hyperparameters or fixed \n",
    "transformations, like the parameters in normalization layers (e.g., Batch Normalization) or predefined embeddings\n",
    "in models.\n",
    "\n",
    "# 2. Placement of the DROPOUT LAYER in CNN architecture:\n",
    "The DROPOUT LAYER is typically placed after one or more convolutional or fully connected layers. It helps in preventing\n",
    "overfitting by randomly dropping a certain fraction of neurons during each training iteration.\n",
    "\n",
    "# 3. Optimal number of hidden layers to stack:\n",
    "The optimal number of hidden layers in a neural network is problem-specific and often requires experimentation. \n",
    "However, a common rule of thumb is to start with a simple architecture and gradually increase the depth (number of layers)\n",
    "if the performance on the validation set does not improve. Too many layers can lead to overfitting.\n",
    "\n",
    "# 4. Number of neurons or filters in each layer:\n",
    "The number of neurons or filters in each layer is also problem-specific and depends on the complexity of the task. It is \n",
    "often determined through experimentation and can vary significantly between different models and applications.\n",
    "\n",
    "# 5. Initial learning rate:\n",
    "The initial learning rate is a hyperparameter that needs to be tuned based on the specific problem and the optimizer being\n",
    "used. Common initial values range from 0.1 to 0.0001, and tuning may be required to find the best learning rate for your\n",
    "model.\n",
    "\n",
    "# 6. Activation function:\n",
    "The choice of activation function (e.g., ReLU, Sigmoid, Tanh) is an essential design choice when building a neural network.\n",
    "It determines the output of a neuron and impacts the networks ability to learn and generalize.\n",
    "\n",
    "# 7. Normalization of data:\n",
    "Data normalization is the process of scaling input features to a standard range (e.g., 0 to 1) or with a mean of 0 and \n",
    "standard deviation of 1. It helps in improving the training process by ensuring that all features contribute equally to\n",
    "the models learning.\n",
    "\n",
    "# 8. Image Augmentation and how it works:\n",
    "Image augmentation is a technique used in computer vision to artificially increase the size of the training dataset by\n",
    "applying various transformations to the original images. These transformations can include rotation, scaling, flipping,\n",
    "and color adjustments. Image augmentation helps improve the models ability to generalize and handle variations in input\n",
    "data.\n",
    "\n",
    "# 9. Decline in learning rate:\n",
    "A decline in learning rate, also known as learning rate scheduling, involves reducing the learning rate during training. \n",
    "This can be done in different ways, like a fixed schedule (e.g., reducing the learning rate by a factor after a certain\n",
    "number of epochs) or dynamically based on the model performance. It helps the model converge more effectively.\n",
    "\n",
    "# 10. Early Stopping Criteria:\n",
    "Early stopping criteria refer to a technique used during training to prevent overfitting. It involves monitoring a metric \n",
    "(e.g., validation loss or accuracy) on a held-out validation set and stopping training when this metric stops improving or \n",
    "starts deteriorating. This helps to avoid training for too long, which can lead to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
