{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a407984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. After each stride-2 conv, why do we double the number of filters?\n",
    "Answer : Doubling the number of filters after each stride-2 convolutional layer helps the network capture increasingly complex and \n",
    "abstract features in the data. As we reduce the spatial dimensions with stride-2 convolutions, we want to maintain the \n",
    "network capacity to extract more fine-grained information. Increasing the number of filters allows the network to learn a \n",
    "richer set of features and helps it adapt to the diminishing spatial resolution.\n",
    "\n",
    "# 2. Why do we use a larger kernel with MNIST (with a simple CNN) in the first convolution?\n",
    "Answer: Using a larger kernel in the first convolutional layer for the MNIST dataset can help capture more global patterns\n",
    "    and structures in the images. MNIST images are relatively small (28x28 pixels), and using a larger kernel (e.g., 5x5) \n",
    "    allows the network to consider a broader context when detecting features and patterns. This can improve the model\n",
    "    ability to recognize handwritten digits by capturing important spatial relationships within the input data.\n",
    "\n",
    "# 3. What data is saved by ActivationStats for each layer?\n",
    "Answer: ActivationStats typically records statistics related to the activations of each layer during training. These \n",
    "    statistics may include mean and standard deviation values of the layer activations. Monitoring these statistics can \n",
    "    help in diagnosing issues like vanishing/exploding gradients or poor weight initialization. They can also be useful \n",
    "    for understanding the distribution of activations, which is essential for techniques like batch normalization.\n",
    "\n",
    "# 4. How do we get a learner's callback after they've completed training?\n",
    "Answer: In the fastai library, you can get a callback after training has completed by using the add_cbs method. For\n",
    "    instance, if you have a Learner object named learn and you want to add a custom callback called my_callback after \n",
    "    training, you can do so with the following code:\n",
    "        learn.add_cbs(my_callback())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What are the drawbacks of activations above zero?\n",
    "\n",
    "Answer: Activations above zero can have several drawbacks, such as the potential for exploding gradients during training, \n",
    "    which can lead to unstable and slow convergence. Additionally, excessively large activations may result in numerical\n",
    "    instability issues, making it challenging to train deep neural networks. It important to use appropriate activation \n",
    "    functions and weight initialization techniques to mitigate these drawbacks.\n",
    "\n",
    "# 6. Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Faster training: Larger batches can take better advantage of hardware parallelism, leading to faster model convergence.\n",
    "Smoother optimization: Larger batches can provide a more stable gradient estimate, reducing the noise in parameter updates.\n",
    "Improved generalization: In some cases, using larger batches can lead to improved generalization by averaging over a larger\n",
    "sample of the training data.\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "Increased memory requirements: Larger batch sizes require more memory, which may limit the use of certain hardware \n",
    "configurations.\n",
    "Learning rate adjustment: Larger batch sizes often require adjustments to the learning rate, which can be more challenging\n",
    "    to tune.\n",
    "Possible loss of fine-grained information: Very large batches may lose the ability to capture fine-grained details in the\n",
    "    data, which can be important for some tasks.\n",
    "\n",
    "# 7. Why should we avoid starting training with a high learning rate?\n",
    "\n",
    "Answer: Starting training with a high learning rate can be problematic because it may lead to rapid and unstable changes \n",
    "    in model weights, making it difficult for the model to converge to a good solution. High initial learning rates can\n",
    "    result in the loss function overshooting the optimal parameter values and failing to find a suitable minimum.\n",
    "    Gradually increasing the learning rate during training (e.g., using learning rate schedules) is a more common practice\n",
    "    to allow the model to explore the loss landscape effectively.\n",
    "\n",
    "# 8. What are the pros of studying with a high rate of learning?\n",
    "\n",
    "Answer: The question seems to be a bit contradictory or unclear. \n",
    "\n",
    "Pros of training with a high learning rate:\n",
    "\n",
    "Faster convergence: High learning rates can lead to faster initial convergence, which can be beneficial when training time is a constraint.\n",
    "Escaping local minima: A high learning rate can help the model escape local minima in the loss landscape, enabling it to find better solutions.\n",
    "Exploration: It encourages the model to explore a wider range of weight values early in training.\n",
    "\n",
    "# 9. Why do we want to end the training with a low learning rate?\n",
    "\n",
    "Answer: Ending training with a low learning rate is beneficial because it allows the model to fine-tune its parameters\n",
    "    and converge to a more precise and stable solution. A low learning rate during the later stages of training helps \n",
    "    the model make small, controlled updates to the weights, reducing the risk of overshooting and providing the \n",
    "    opportunity for the model to settle into a local minimum or minimum of the loss function. This can result in improved\n",
    "    model performance and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
