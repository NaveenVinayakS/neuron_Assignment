{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "# a.Number of Hidden Layers:\n",
    "You can experiment with different numbers of hidden layers and find the optimal architecture through trial and error.\n",
    "Example in Python (using TensorFlow):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# b. Network Architecture (Network Depth):\n",
    "\n",
    "Similar to the number of hidden layers, you can experiment with different network architectures and choose the one that\n",
    "performs best on your problem.\n",
    "\n",
    "# c. Each Layer's Number of Neurons (Layer Width):\n",
    "\n",
    "You can vary the number of neurons in each layer to find the optimal layer width.\n",
    "Example\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "# d. Form of Activation:\n",
    "\n",
    "Experiment with different activation functions like ReLU, Sigmoid, Tanh, etc.\n",
    "Example\n",
    "    tf.keras.layers.Dense(64, activation='tanh')\n",
    "\n",
    "# e. Optimization and Learning:\n",
    "\n",
    "Choose an optimizer (e.g., SGD, Adam, RMSprop) and experiment with learning rates.\n",
    "Example:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# f. Learning Rate and Decay Schedule:\n",
    "\n",
    "Use learning rate schedules or adaptative learning rates like Learning Rate Annealing or Cyclical Learning Rates.\n",
    "Example:\n",
    "    scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
    "\n",
    "# g. Mini Batch Size:\n",
    "\n",
    "Adjust the mini-batch size for training. Smaller batch sizes can sometimes lead to better convergence.\n",
    "Example:\n",
    "    batch_size = 32\n",
    "\n",
    "# h. Algorithms for Optimization:\n",
    "\n",
    "Experiment with different optimization algorithms, as mentioned in point 5.\n",
    "\n",
    "# i. Number of Epochs (and Early Stopping Criteria):\n",
    "\n",
    "Train your model for a set number of epochs or use early stopping to monitor validation performance and stop training when it plateaus.\n",
    "Example:\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# j. Overfitting Avoidance by Regularization Techniques:\n",
    "\n",
    "Use techniques like L2 regularization, dropout, and data augmentation.\n",
    "Example:\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd553f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2 Normalization:\n",
    "As mentioned in point 10, you can apply L2 regularization to the layers to encourage smaller weight values.\n",
    "\n",
    "Dropout Layers:\n",
    "Add dropout layers to your network to prevent overfitting.\n",
    "Example :\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "Data Augmentation:\n",
    "Use data augmentation techniques, especially for image data, to create variations of the training data and reduce overfitting.\n",
    "Example :\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
