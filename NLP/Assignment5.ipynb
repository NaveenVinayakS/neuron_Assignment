{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequence-to-sequence models:\n",
    "Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture designed for tasks involving sequential data, where the input and output can have varying lengths. They consist of two main components: an encoder, which processes the input sequence and produces a fixed-length representation, and a decoder, which generates the output sequence based on the encoder's representation. Seq2Seq models are widely used in machine translation, text summarization, speech recognition, and more.\n",
    "\n",
    "Problems with Vanilla RNNs:\n",
    "Vanilla RNNs suffer from the vanishing gradient problem, where gradients become very small during training, making it difficult to learn long-range dependencies. They also have the exploding gradient problem when gradients become very large. These issues limit their ability to capture meaningful information from long sequences.\n",
    "\n",
    "Gradient clipping:\n",
    "Gradient clipping is a technique used during training to prevent exploding gradients. It involves setting a threshold for the gradients, and if they exceed this threshold, they are scaled down proportionally to ensure they don't become too large. This helps stabilize training and prevents the model from diverging.\n",
    "\n",
    "Attention mechanism:\n",
    "Attention is a mechanism used in neural networks to focus on specific parts of the input sequence when producing an output. Instead of relying solely on a fixed-length representation from the encoder, the attention mechanism allows the model to weigh different parts of the input sequence differently based on their relevance to the current decoding step. This improves the model's ability to capture context and dependencies in the input sequence.\n",
    "\n",
    "Conditional random fields (CRFs):\n",
    "Conditional random fields are a type of probabilistic graphical model used for structured prediction tasks, such as part-of-speech tagging, named entity recognition, and sequence labeling. CRFs model the conditional probability distribution of a sequence of labels given the input sequence, taking into account label dependencies and global consistency.\n",
    "\n",
    "Self-attention:\n",
    "Self-attention is a mechanism that allows an element in a sequence to attend to other elements in the same sequence. It is used in models like the Transformer to capture dependencies between different positions in a sequence. Self-attention computes weighted sums of all positions in the sequence to generate context-aware representations for each position.\n",
    "\n",
    "Bahdanau Attention:\n",
    "Bahdanau Attention is an attention mechanism introduced in the context of Seq2Seq models. It allows the decoder to focus on different parts of the source sequence at each decoding step. It computes attention weights based on the alignment between the current decoder hidden state and the encoder hidden states, providing context information for the output generation.\n",
    "\n",
    "Language Model:\n",
    "A language model is a statistical model that predicts the probability of a sequence of words or tokens in a language. It is trained on large text corpora and can be used for various natural language processing tasks, such as text generation, machine translation, and speech recognition.\n",
    "\n",
    "Multi-Head Attention:\n",
    "Multi-Head Attention is an extension of self-attention used in the Transformer model. It involves multiple sets of attention weights (heads) that allow the model to focus on different aspects of the input sequence simultaneously. Multi-Head Attention helps the model capture different types of dependencies and improves its representational power.\n",
    "\n",
    "Bilingual Evaluation Understudy (BLEU):\n",
    "BLEU is a metric used to evaluate the quality of machine-generated text, especially in machine translation tasks. It measures the similarity between a machine-generated translation and one or more reference translations. BLEU calculates a score between 0 and 1, with higher scores indicating better translation quality. It's widely used in research and development of machine translation systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
