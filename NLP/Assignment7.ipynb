{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ec5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Architecture of BERT:\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based model designed for various natural language processing tasks. It consists of an encoder stack, typically with 12 or 24 layers. BERT uses a multi-head self-attention mechanism to capture context from both left and right sides of each word, allowing it to understand context bidirectionally. BERT's architecture enables it to learn rich contextual embeddings, which can be fine-tuned for specific NLP tasks. For example, BERT has become the foundation for tasks like text classification, named entity recognition, and question answering.\n",
    "\n",
    "Masked Language Modeling (MLM):\n",
    "MLM is a pre-training task used to train BERT. In MLM, a portion of the input tokens is randomly replaced with a special [MASK] token, and the model's objective is to predict the original tokens. For example, given the sentence \"I love [MASK] cats,\" BERT should predict the masked word as \"my\" based on context.\n",
    "\n",
    "Next Sentence Prediction (NSP):\n",
    "NSP is another pre-training task for BERT. In NSP, two sentences are provided as input, and the model predicts whether the second sentence logically follows the first one. For example, given \"I love cats.\" and \"They are so cute!\", BERT should predict that the second sentence logically follows the first.\n",
    "\n",
    "Matthews Evaluation:\n",
    "Matthews evaluation is used to assess the quality of binary classification models, particularly when dealing with imbalanced datasets. It calculates the Matthews Correlation Coefficient (MCC), which takes into account true positives, true negatives, false positives, and false negatives. MCC provides a balanced measure of classification performance and is especially useful when one class is significantly smaller than the other.\n",
    "\n",
    "Matthews Correlation Coefficient (MCC):\n",
    "MCC is a measure of the quality of binary classification models. It ranges from -1 (completely incorrect predictions) to +1 (perfect predictions), with 0 indicating random predictions. The formula for MCC is:\n",
    "\n",
    "Semantic Role Labeling:\n",
    "Semantic Role Labeling (SRL) is a natural language processing task that involves identifying the semantic roles of words or phrases in a sentence, often with respect to a predicate (usually a verb). For example, in the sentence \"She ate a delicious cake,\" SRL would identify \"She\" as the agent, \"ate\" as the predicate, and \"a delicious cake\" as the patient.\n",
    "\n",
    "Fine-tuning vs. Pretraining Time for BERT:\n",
    "Fine-tuning a pre-trained BERT model takes less time than the initial pretraining because the pretraining phase involves training on a massive amount of text data, which can take weeks or even months. Fine-tuning, on the other hand, uses the pre-trained model as a starting point and adapts it to specific downstream tasks, requiring significantly less time, often just a few hours or days.\n",
    "\n",
    "Recognizing Textual Entailment (RTE):\n",
    "RTE is a natural language understanding task where the goal is to determine if one text (the hypothesis) logically follows or contradicts another text (the premise). It's a binary classification problem. For example, given the premise \"The cat is on the mat\" and the hypothesis \"There is a cat indoors,\" the task is to recognize that the hypothesis is entailed by the premise.\n",
    "\n",
    "Decoder Stack of GPT Models:\n",
    "The decoder stack in GPT (Generative Pre-trained Transformer) models is a part of the architecture that is responsible for generating text. It typically consists of multiple layers of decoder blocks. Each block includes a multi-head self-attention mechanism and a feedforward neural network. The decoder stack takes in a context (e.g., the output from the encoder stack or previous generated tokens) and produces the next token in a sequence. In autoregressive language models like GPT, the decoder stack generates text one token at a time, conditioning each token on the previously generated ones.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
