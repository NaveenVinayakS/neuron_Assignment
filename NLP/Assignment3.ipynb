{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c27665",
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Architecture of RNN Cell:\n",
    "The basic architecture of an RNN cell consists of an input, a hidden state, and an output. It takes an input vector and the previous time step's hidden state as inputs and produces an output and a new hidden state. The output and new hidden state are computed using activation functions. The hidden state retains information from previous time steps, making RNNs suitable for sequential data processing.\n",
    "\n",
    "Backpropagation Through Time (BPTT):\n",
    "BPTT is a training algorithm for RNNs that extends backpropagation to sequences. It computes gradients of the loss function with respect to the model's parameters by unfolding the RNN through time. It then updates the model's parameters using these gradients.\n",
    "\n",
    "Vanishing and Exploding Gradients:\n",
    "Vanishing gradients occur when the gradients during backpropagation become very small, causing the model to have difficulty learning long-range dependencies. Exploding gradients occur when gradients become very large, leading to unstable training. These issues can affect traditional RNNs and are mitigated by variants like LSTMs and GRUs.\n",
    "\n",
    "Long Short-Term Memory (LSTM):\n",
    "LSTM is a type of RNN cell designed to address the vanishing gradient problem. It has specialized gating mechanisms (input, forget, and output gates) that control the flow of information, allowing it to capture long-term dependencies. LSTMs are well-suited for tasks involving sequences with long-range dependencies.\n",
    "\n",
    "Gated Recurrent Unit (GRU):\n",
    "GRU is another variant of RNN that addresses the vanishing gradient problem. It simplifies the architecture compared to LSTM by using fewer gates (reset and update gates) while still capturing long-term dependencies. GRUs are computationally efficient and often used when resources are limited.\n",
    "\n",
    "Peephole LSTM:\n",
    "Peephole LSTM is an extension of the standard LSTM architecture that allows the gates to access the cell state directly. In addition to the input and hidden states, it uses information from the cell state to make gating decisions.\n",
    "\n",
    "Bidirectional RNNs:\n",
    "Bidirectional RNNs process sequences in both forward and backward directions. This helps capture information from past and future time steps simultaneously, making them useful for tasks like speech recognition and sentiment analysis.\n",
    "\n",
    "Gates of LSTM with Equations:\n",
    "The LSTM has three gates:\n",
    "\n",
    "Input Gate (i): Controls how much new information is added to the cell state.\n",
    "Forget Gate (f): Controls what information is discarded or forgotten from the cell state.\n",
    "Output Gate (o): Controls how much of the cell state is used to compute the output.\n",
    "\n",
    "Bidirectional LSTM (BiLSTM):\n",
    "BiLSTM is an extension of LSTM that processes sequences in both forward and backward directions. It combines the outputs of two LSTMs, one processing the sequence forwards and the other backwards, to capture bidirectional context information.\n",
    "\n",
    "Bidirectional GRU (BiGRU):\n",
    "Similar to BiLSTM, BiGRU is a bidirectional variant of the GRU cell. It processes input sequences in both directions to capture bidirectional dependencies in the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
