{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341aff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "One-Hot Encoding:\n",
    "One-Hot Encoding is a technique used in natural language processing and machine learning to convert categorical data, such as words or labels, into binary vectors. Each category is represented by a vector where all elements are 0 except for one, which is 1. For example, consider a list of fruits: [\"apple\", \"banana\", \"orange\"]. One-hot encoding would represent \"banana\" as [0, 1, 0].\n",
    "\n",
    "Bag of Words (BoW):\n",
    "Bag of Words is a text representation technique where a document is represented as a collection of its words, disregarding grammar and word order. It creates a sparse vector where each element represents the count of a unique word in the document. For example, the sentence \"I love cats, and I love dogs\" would be represented as {\"I\": 2, \"love\": 2, \"cats\": 1, \"and\": 1, \"dogs\": 1}.\n",
    "\n",
    "Bag of N-Grams:\n",
    "Bag of N-Grams extends the concept of BoW by considering sequences of N contiguous words (N-grams) instead of single words. For example, with N=2, the sentence \"I love cats, and I love dogs\" would be represented as {\"I love\": 2, \"love cats\": 1, \"cats and\": 1, \"and I\": 1, \"I love\": 1, \"love dogs\": 1}.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "TF-IDF is a technique that evaluates the importance of words in a document relative to a corpus of documents. It combines the term frequency (TF), which measures how often a word appears in a document, with the inverse document frequency (IDF), which measures how unique a word is across the entire corpus. Words that are common in a document but rare in the corpus get higher TF-IDF scores.\n",
    "\n",
    "OOV Problem (Out of Vocabulary Problem):\n",
    "The OOV problem occurs when a word in a text or document is not found in the vocabulary of a language model or NLP system. For example, if you have a language model trained on English text, and you input a sentence in a language it hasn't seen before, it will encounter OOV words.\n",
    "\n",
    "Word Embeddings:\n",
    "Word embeddings are vector representations of words in a continuous vector space. They capture semantic relationships between words based on their context in a large text corpus. Examples of word embeddings include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "Continuous Bag of Words (CBOW):\n",
    "CBOW is a word embedding model that predicts a target word based on its context words in a given sentence. It aims to learn word embeddings by minimizing the prediction error of the target word using the context words.\n",
    "\n",
    "SkipGram:\n",
    "SkipGram is another word embedding model that works in the opposite way of CBOW. Instead of predicting the target word from context words, SkipGram predicts context words from a target word. It is often used when you want to generate word embeddings with fine-grained semantic information.\n",
    "\n",
    "Glove Embeddings:\n",
    "GloVe (Global Vectors for Word Representation) is a word embedding technique that combines the advantages of count-based and prediction-based methods. It uses global word co-occurrence statistics to learn word vectors. For example, it can capture that the word \"king\" is related to \"queen\" in terms of semantics because they often co-occur in similar contexts across a large corpus of text.S\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
