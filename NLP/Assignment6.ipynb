{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40056f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vanilla Autoencoders:\n",
    "Vanilla autoencoders are a type of neural network architecture used for unsupervised learning and dimensionality reduction. They consist of an encoder network that compresses the input data into a lower-dimensional representation (encoding) and a decoder network that reconstructs the input data from this encoding. The objective is to minimize the reconstruction error, which encourages the model to learn a compact representation of the data. Example: Reducing the dimensionality of an image while preserving its key features.\n",
    "\n",
    "Sparse Autoencoders:\n",
    "Sparse autoencoders are similar to vanilla autoencoders but with an added constraint on the sparsity of the encoding. They encourage the encoder to produce sparse representations, meaning that only a few encoding values should be significantly different from zero. Sparse autoencoders are useful for feature selection and learning disentangled representations. Example: Identifying important features in an image for classification.\n",
    "\n",
    "Denoising Autoencoders:\n",
    "Denoising autoencoders are trained to reconstruct clean data from noisy input. During training, random noise or corruption is added to the input data, and the model learns to denoise and recover the original data. Denoising autoencoders are robust to noisy input and can be used for data denoising and inpainting. Example: Removing noise from images or restoring missing parts of a corrupted text.\n",
    "\n",
    "Convolutional Autoencoders:\n",
    "Convolutional autoencoders use convolutional neural network (CNN) layers in the encoder and decoder to handle data with grid-like structures, such as images. They are effective for capturing spatial hierarchies and patterns in data. Example: Image compression or feature extraction from images.\n",
    "\n",
    "Stacked Autoencoders:\n",
    "Stacked autoencoders are composed of multiple layers of autoencoders stacked on top of each other. The output of one layer serves as the input to the next. Stacked autoencoders can capture hierarchical representations in data, making them suitable for complex feature learning and representation. Example: Deep learning for image recognition.\n",
    "\n",
    "Generating Sentences using LSTM Autoencoders:\n",
    "To generate sentences using LSTM autoencoders, you can train the model on a dataset of sentences and use it to encode and decode sentences. During decoding, you start with a seed or initial word and repeatedly generate the next word using the LSTM decoder and the previous word. This process continues until an end-of-sentence token is generated. Example: Generating coherent and contextually relevant text in chatbots or text generation tasks.\n",
    "\n",
    "Extractive Summarization:\n",
    "Extractive summarization is a text summarization technique that selects and extracts the most important sentences or phrases from a document to create a summary. It doesn't generate new sentences but rather identifies and ranks existing content based on its relevance to the main ideas in the text. Example: Automatically extracting key sentences from news articles for a summary.\n",
    "\n",
    "Abstractive Summarization:\n",
    "Abstractive summarization is a text summarization technique that aims to generate a summary that may not be a verbatim extraction from the source document. Instead, it uses natural language generation to paraphrase and create concise summaries in a more human-like manner. Example: Generating a concise news headline from a lengthy article.\n",
    "\n",
    "Beam Search:\n",
    "Beam search is a search algorithm used in sequence generation tasks, such as machine translation and text generation. It explores multiple potential sequence paths concurrently, maintaining a \"beam\" of the most likely candidates at each decoding step. Beam search is used to find the most probable sequence based on a scoring criterion, like the likelihood of a sequence given by a language model.\n",
    "\n",
    "Length Normalization:\n",
    "Length normalization is a technique used in sequence generation tasks to mitigate the bias towards longer sequences. It involves dividing the score or probability of a generated sequence by a length-related factor, such as the length of the sequence raised to a certain power. This helps prevent shorter sequences from being favored in scoring.\n",
    "\n",
    "Coverage Normalization:\n",
    "Coverage normalization is a technique used in abstractive summarization to ensure that the generated summary covers all the important content from the source document. It keeps track of which parts of the source document have been attended to during generation and encourages the model to attend to the uncovered portions.\n",
    "\n",
    "ROUGE Metric Evaluation:\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used for evaluating the quality of machine-generated text summaries, machine translation, and other natural language processing tasks. ROUGE measures the overlap and similarity between the generated output and one or more reference texts. It includes various metrics such as ROUGE-N (n-gram overlap) and ROUGE-L (longest common subsequence), which help assess the quality and fluency of generated text in comparison to human-written references.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
