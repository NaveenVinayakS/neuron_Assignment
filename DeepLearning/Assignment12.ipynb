{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "How does unsqueeze help us to solve certain broadcasting problems?\n",
    "unsqueeze adds a new dimension to a tensor, which can be crucial for broadcasting operations. It helps align the dimensions of two tensors for element-wise operations.\n",
    "\n",
    "import torch\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([10, 20])\n",
    "result = tensor1.unsqueeze(1) + tensor2  # Broadcasting tensor1 to shape (3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780adefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "How can we use indexing to do the same operation as unsqueeze?\n",
    "You can achieve the same result using indexing like this:\n",
    "    \n",
    "import torch\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([10, 20])\n",
    "result = tensor1[:, None] + tensor2  # Using indexing to add a new axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad2da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "How do we show the actual contents of the memory used for a tensor?\n",
    "You can use the .numpy() method to convert a PyTorch tensor to a NumPy array and then print it to see the actual values. For example:\n",
    "    \n",
    "import torch\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "numpy_array = tensor.numpy()\n",
    "print(numpy_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "When adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix?\n",
    "The elements of the vector are added to each column of the matrix. Broadcasting extends the vector along the rows to match the shape of the matrix.\n",
    "\n",
    "Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "No, broadcasting and expand_as do not result in increased memory use because they do not create new copies of the data. They operate on the original tensor views, which share memory. This makes them memory-efficient.\n",
    "\n",
    "Implement matmul using Einstein summation.\n",
    "Here's an example of implementing matrix multiplication using Einstein summation:\n",
    "\n",
    "import torch\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "result = torch.einsum('ij,jk->ik', A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "What does a repeated index letter represent on the lefthand side of einsum?\n",
    "A repeated index letter represents a summation over that index. In the Einstein summation notation, repeated indices imply a summation operation.\n",
    "\n",
    "What are the three rules of Einstein summation notation? Why?\n",
    "The three rules of Einstein summation notation are:\n",
    "\n",
    "Repeated indices are implicitly summed over.\n",
    "Indices can appear at most twice in any term.\n",
    "Each term must have the same number of indices as its corresponding operand.\n",
    "These rules ensure that the notation represents valid tensor operations and simplifies complex expressions.\n",
    "What are the forward pass and backward pass of a neural network?\n",
    "The forward pass is the process of propagating input data through a neural network to generate predictions or outputs. Each layer in the network computes weighted sums and applies activation functions sequentially. The forward pass calculates the predicted output of the network.\n",
    "\n",
    "The backward pass, also known as backpropagation, is the process of computing gradients of the loss function with respect to the network's parameters. These gradients are used in optimization algorithms (e.g., gradient descent) to update the model's weights during training.\n",
    "\n",
    "Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "Intermediate activations are stored during the forward pass because they are required during the backward pass for gradient computation. These activations are used to calculate gradients with respect to the loss function and update the model's parameters during training via backpropagation.\n",
    "\n",
    "What is the downside of having activations with a standard deviation too far away from 1?\n",
    "Activations with a standard deviation too far from 1 can lead to issues during training, such as vanishing or exploding gradients. Vanishing gradients make it challenging for the network to learn, while exploding gradients can lead to unstable training. Keeping activations close to a standard deviation of 1 helps stabilize and speed up training.\n",
    "\n",
    "How can weight initialization help avoid this problem?\n",
    "Proper weight initialization techniques, such as Xavier/Glorot initialization or He initialization, can help avoid the problem of activations with a standard deviation too far from 1. These techniques initialize the model's weights in a way that encourages activations to stay within a reasonable range during training, which mitigates gradient issues and accelerates convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
