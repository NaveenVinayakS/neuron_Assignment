{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c36e17",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3519756083.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Write the Python code to implement a single neuron.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Write the Python code to implement a single neuron.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def single_neuron(input_data, weights, bias):\n",
    "    weighted_sum = np.dot(input_data, weights) + bias\n",
    "    output = 1 / (1 + np.exp(-weighted_sum))  # Sigmoid activation function\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafd27e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1736612608.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Write the Python code to implement ReLU.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Write the Python code to implement ReLU.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dense_layer(input_data, weights, bias):\n",
    "    output = np.dot(input_data, weights) + bias\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
    "\n",
    "def dense_layer_plain(input_data, weights, bias):\n",
    "    output = [sum(w * x for w, x in zip(weights, input_data)) + bias]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e910d2c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (2584894796.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    The hidden size of a layer refers to the number of neurons or units in that layer. It represents the dimensionality of the layer's output.\u001b[0m\n\u001b[1;37m                                                                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "What is the \"hidden size\" of a layer?\n",
    "\n",
    "The hidden size of a layer refers to the number of neurons or units in that layer. It represents the dimensionality of the layer's output.\n",
    "What does the t method do in PyTorch?\n",
    "\n",
    "The t method in PyTorch is used to transpose a tensor, swapping its dimensions. For example, if you have a 2D tensor, calling t on it will transpose its rows and columns.\n",
    "Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "Matrix multiplication written in plain Python is slow because it involves nested loops for element-wise multiplication and summation. These loops are not optimized for performance, especially for large matrices. Libraries like NumPy or specialized hardware accelerators like GPUs are much faster for matrix multiplication due to optimized implementations.\n",
    "In matmul, why is ac==br?\n",
    "\n",
    "In matrix multiplication, the dimensions of the matrices must satisfy the rule that the number of columns in the first matrix (a) must be equal to the number of rows in the second matrix (b) for the multiplication to be defined. That's why it's written as ac==br, where a has shape (a_rows, a_columns) and b has shape (b_rows, b_columns).\n",
    "In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "You can measure the time taken for a single cell to execute in a Jupyter Notebook using the %timeit magic command. \n",
    "\n",
    "\n",
    "What is elementwise arithmetic?\n",
    "\n",
    "Elementwise arithmetic refers to performing arithmetic operations (e.g., addition, subtraction, multiplication, division) on corresponding elements of two or more arrays or tensors. Each element in one array is combined with the corresponding element(s) in another array, resulting in an output array of the same shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "result = torch.all(a > b)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321deacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "A rank-0 tensor is a tensor with zero dimensions, essentially representing a single scalar value. You can convert it to a plain Python data type using the .item() method. For example:\n",
    "    \n",
    "import torch\n",
    "\n",
    "scalar_tensor = torch.tensor(42)\n",
    "scalar_value = scalar_tensor.item()\n",
    "print(scalar_value)  # This will print 42 as a Python integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "Elementwise arithmetic allows us to perform operations on individual elements of matrices or tensors in parallel. When applied to matmul (matrix multiplication), elementwise operations can be used to optimize certain parts of the matrix multiplication algorithm, making it more efficient, especially on hardware with parallel processing capabilities.\n",
    "What are the broadcasting rules?\n",
    "\n",
    "Broadcasting is a mechanism in NumPy and other libraries that allows for elementwise operations on arrays or tensors with different shapes, as long as certain broadcasting rules are met. The rules are as follows:\n",
    "If the arrays have a different number of dimensions, pad the smaller shape with ones on the left side until they have the same number of dimensions.\n",
    "Compare the dimensions element-wise. Two dimensions are compatible when either they are equal or one of them is 1.\n",
    "If these conditions are met, broadcasting will be applied, and the smaller array will be broadcast to match the shape of the larger array for elementwise operations.\n",
    "What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "expand_as is a method in PyTorch that allows you to expand the dimensions of a tensor to match the shape of another tensor. It is often used to ensure compatibility for elementwise operations or broadcasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
