{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfd9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initializing Weights with He Initialization:\n",
    "\n",
    "Initializing all weights to the same value, even if selected randomly using He initialization, is generally not recommended. While He initialization helps prevent the vanishing gradient problem by setting initial weights with variance proportional to the number of input units, initializing all weights to the same value could lead to symmetrical neurons.\n",
    "Symmetrical neurons, where all neurons in a layer behave similarly, hinder the learning process. Neural networks rely on diversity in neuron behaviors to learn useful features. Random initialization breaks symmetry and allows neurons to specialize in detecting different patterns.\n",
    "Initializing Bias Terms to 0:\n",
    "\n",
    "Initializing bias terms to 0 is a common practice and is generally acceptable. Unlike weights, biases don't have the same symmetry-breaking issue. Bias terms help shift the activation function, allowing the model to fit the data better.\n",
    "It is worth noting that there are variants of initialization techniques (e.g., bias initialization with small positive values) that can be used if necessary, but initializing biases to 0 is a reasonable default.\n",
    "Advantages of SELU Activation over ReLU:\n",
    "\n",
    "1. Avoiding Dead Neurons: SELU can avoid the \"dying ReLU\" problem by allowing some neurons to have non-zero gradients for inputs below 0, reducing the likelihood of dead neurons.\n",
    "2. Self-Normalization: SELU activations can self-normalize, helping in training very deep networks without the need for batch normalization, which can simplify architectures and reduce training time.\n",
    "3. Improved Generalization: SELU has been shown to outperform ReLU on certain tasks, leading to better generalization and lower test error.\n",
    "Activation Function Selection:\n",
    "\n",
    "SELU: Useful for deep networks where self-normalization is desired. However, it may not perform well on all tasks.\n",
    "Leaky ReLU (and variants): Suitable for deep networks to mitigate the dying ReLU problem. Variants like Parametric ReLU (PReLU) and Exponential Linear Unit (ELU) offer better performance in some cases.\n",
    "ReLU: A widely used default choice for hidden layers due to its simplicity and efficiency. Can be sensitive to weight initialization.\n",
    "tanh: Suitable for hidden layers when inputs are centered around zero. It squashes values to the range (-1, 1).\n",
    "Logistic (Sigmoid): Useful in binary classification output layers to produce values between 0 and 1.\n",
    "Softmax: Commonly used in multi-class classification output layers to produce probability distributions over classes.\n",
    "Effect of High Momentum in SGD:\n",
    "\n",
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) can lead to slow convergence and potential oscillations during training. It essentially causes the optimizer to \"accumulate\" past gradients to a high degree, making it less responsive to recent gradient information.\n",
    "High momentum values may cause the optimizer to overshoot and take longer to converge to the minimum of the loss function. It can also result in noisy updates if gradients fluctuate.\n",
    "Producing a Sparse Model:\n",
    "\n",
    "Weight Pruning: Identifying and setting small or less important weights to zero, effectively reducing the model's connectivity.\n",
    "L1 Regularization (Lasso): Adding L1 regularization to the loss function encourages weight sparsity by penalizing large weights.\n",
    "Dropout: Dropout can be seen as a technique for creating a sparse model during training. During inference, it produces an ensemble of models to make predictions.\n",
    "Effect of Dropout on Training and Inference:\n",
    "\n",
    "Dropout can slow down training because it randomly deactivates neurons during each training iteration, effectively reducing the effective network size. However, it can lead to better generalization and prevent overfitting.\n",
    "\n",
    "During inference (making predictions on new instances), dropout is typically turned off (dropping out with a probability of 0), so it does not slow down inference time. Predictions are made using the full network.\n",
    "\n",
    "MC Dropout (Monte Carlo Dropout): During inference, MC Dropout performs multiple forward passes with dropout enabled and averages the predictions. While this may slow down inference slightly, it can provide better uncertainty estimates and more reliable predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
