{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Applications for Sequence-to-Sequence RNN:\n",
    "\n",
    "Machine Translation: Translating sequences of words from one language to another.\n",
    "Speech Recognition: Converting spoken language into text.\n",
    "Video Captioning: Generating textual descriptions for video frames.\n",
    "Text Summarization: Producing concise summaries of longer text documents.\n",
    "Conversational Agents: Generating responses in chatbots and virtual assistants.\n",
    "Applications for Sequence-to-Vector RNN:\n",
    "\n",
    "Sentiment Analysis: Classifying the sentiment of a text sequence (e.g., movie reviews).\n",
    "Document Classification: Assigning a category to a document.\n",
    "Text Generation: Creating coherent text based on an input sequence.\n",
    "Speech Emotion Recognition: Detecting emotions from spoken text.\n",
    "Applications for Vector-to-Sequence RNN:\n",
    "\n",
    "Image Captioning: Generating textual descriptions for images.\n",
    "Language Modeling: Predicting the next word in a sentence given preceding words.\n",
    "Handwriting Synthesis: Converting vectorized input (e.g., strokes) into handwritten text.\n",
    "Dimensions of RNN Inputs and Outputs:\n",
    "\n",
    "Inputs to an RNN layer typically have three dimensions: (batch_size, time_steps, input_features).\n",
    "batch_size: The number of sequences processed in each batch.\n",
    "time_steps: The length of each sequence (number of time steps).\n",
    "input_features: The number of features at each time step.\n",
    "Outputs from an RNN layer also have three dimensions: (batch_size, time_steps, output_features).\n",
    "batch_size: Same as input.\n",
    "time_steps: Typically, it matches the input time steps but can differ.\n",
    "output_features: The number of features at each time step.\n",
    "return_sequences=True in RNN Layers:\n",
    "\n",
    "In a deep sequence-to-sequence RNN, return_sequences=True should be set for all hidden RNN layers to pass the sequence information to subsequent layers.\n",
    "In a sequence-to-vector RNN, only the last RNN layer should have return_sequences=False to output a single vector summarizing the sequence.\n",
    "Forecasting Daily Time Series:\n",
    "\n",
    "For forecasting the next seven days of a daily univariate time series, you can use a sequence-to-vector RNN. The RNN processes historical data and outputs a single vector representing the predictions for the next seven days.\n",
    "Difficulties in Training RNNs and Handling Them:\n",
    "\n",
    "Vanishing Gradients: Addressed by using activation functions like ReLU or specialized RNN cells like LSTM or GRU.\n",
    "Exploding Gradients: Mitigated using gradient clipping.\n",
    "Long-Term Dependencies: LSTM and GRU cells are designed to capture longer-term dependencies.\n",
    "Overfitting: Use dropout, early stopping, or regularization techniques.\n",
    "Training Time: Reduce the sequence length, batch size, or use GPU acceleration for faster training.\n",
    "LSTM Cell Architecture:\n",
    "\n",
    "LSTM (Long Short-Term Memory) cells have three gates: Forget Gate, Input Gate, and Output Gate.\n",
    "Each gate is controlled by a sigmoid activation function and has weights that are learned during training.\n",
    "LSTM cells use a cell state that can carry information over long sequences, allowing them to capture long-term dependencies.\n",
    "1D Convolutional Layers in RNN:\n",
    "\n",
    "1D convolutional layers are used in RNNs to capture local patterns and relationships within sequences.\n",
    "They can act as feature extractors, helping the RNN to learn relevant features from the input sequences.\n",
    "1D convolutions can be especially useful for tasks like text classification, where capturing n-gram patterns is important.\n",
    "Neural Network Architecture for Video Classification:\n",
    "\n",
    "For video classification, you can use 3D convolutional neural networks (CNNs) like C3D or 2D CNNs applied to each frame followed by RNN layers to capture temporal dependencies.\n",
    "Alternatively, you can use 3D CNN architectures like I3D designed for spatiotemporal video analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e755f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets.\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the SketchRNN dataset\n",
    "dataset_name = 'sketchrnn/quickdraw'\n",
    "(train_dataset, test_dataset), info = tfds.load(\n",
    "    name=dataset_name,\n",
    "    split=['train[:80%]', 'test[80%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_fn(sketch, label):\n",
    "    # Convert sketches to tensors and normalize\n",
    "    sketch = tf.convert_to_tensor(sketch['drawing'], dtype=tf.float32) / 255.0\n",
    "    # Resize sketches to a fixed size (optional)\n",
    "    sketch = tf.image.resize(sketch, (64, 64))\n",
    "    return sketch, label\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "train_dataset = train_dataset.map(preprocess_fn)\n",
    "test_dataset = test_dataset.map(preprocess_fn)\n",
    "\n",
    "# Define the classification model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(info.features['label'].num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size).shuffle(1000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "epochs = 10\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
