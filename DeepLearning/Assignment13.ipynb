{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e91d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "Logistic Regression is preferable because it can provide probabilistic predictions and can handle non-linearly separable data. To make a Perceptron equivalent to Logistic Regression, you can apply the sigmoid activation function to its output and use a loss function like binary cross-entropy. Here's Python code:\n",
    "\n",
    "# Perceptron with Sigmoid activation and Binary Cross-Entropy loss\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def perceptron(X, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Training loop (update weights using gradient descent)\n",
    "def train_perceptron(X, y, learning_rate, epochs):\n",
    "    weights = np.random.rand(X.shape[1])  # Initialize weights\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = perceptron(X, weights)\n",
    "        loss = binary_cross_entropy(y, y_pred)\n",
    "        gradient = np.dot(X.T, (y_pred - y)) / X.shape[0]\n",
    "        weights -= learning_rate * gradient\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "\n",
    "The logistic (sigmoid) activation function was used because it introduces non-linearity, allowing neural networks to approximate complex functions. It's differentiable, which is crucial for backpropagation. However, it suffers from vanishing gradients, which later led to the development of more effective activation functions like ReLU.\n",
    "\n",
    "3. Name three popular activation functions. Can you draw them?\n",
    "\n",
    "Three popular activation functions are:\n",
    "\n",
    "ReLU (Rectified Linear Unit):\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Sigmoid:\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "Tanh (Hyperbolic Tangent):\n",
    "f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "Unfortunately, I can't draw them directly, but these functions exhibit specific shapes. ReLU is a linear ramp from 0 for positive values. Sigmoid is an S-shaped curve between 0 and 1, while Tanh is an S-shaped curve between -1 and 1.\n",
    "\n",
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons, all using the ReLU activation function.\n",
    "\n",
    "Input matrix X shape: (batch_size, 10)\n",
    "Hidden layer weight vector Wh shape: (10, 50)\n",
    "Hidden layer bias vector bh shape: (50,)\n",
    "Output layer weight vector Wo shape: (50, 3)\n",
    "Output layer bias vector bo shape: (3,)\n",
    "Network's output matrix Y shape: (batch_size, 3)\n",
    "The equation for computing the network's output Y as a function of X, Wh, bh, Wo, and bo is as follows (assuming a single data sample):\n",
    "\n",
    "hidden_output = np.maximum(0, np.dot(X, Wh) + bh)\n",
    "Y = np.dot(hidden_output, Wo) + bo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How many neurons do you need in the output layer for email classification (spam or ham)? What activation function should you use? For MNIST classification, how many neurons are needed, and what activation function?\n",
    "\n",
    "For email classification (binary classification, spam or ham), you need 1 neuron in the output layer with the sigmoid activation function.\n",
    "\n",
    "For MNIST classification (multi-class, 10 digits), you need 10 neurons in the output layer with the softmax activation function.\n",
    "\n",
    "6. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "Backpropagation is a supervised learning algorithm used to train neural networks. It works by iteratively adjusting the network's weights and biases based on the gradient of a loss function with respect to these parameters. The process involves forward and backward passes through the network:\n",
    "\n",
    "Forward Pass: Compute the network's predictions and the corresponding loss.\n",
    "Backward Pass (Backpropagation): Compute gradients of the loss with respect to each parameter using the chain rule and update the parameters using gradient descent.\n",
    "Reverse-mode autodiff is a mathematical technique for efficiently computing gradients in computational graphs. Backpropagation is a specific application of reverse-mode autodiff in the context of neural network training. They are essentially the same thing, with backpropagation being the algorithmic implementation of reverse-mode autodiff for neural networks.\n",
    "\n",
    "7. List all the hyperparameters you can tweak in an MLP. If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "Hyperparameters in an MLP include:\n",
    "\n",
    "Learning rate\n",
    "Number of hidden layers\n",
    "Number of neurons in each hidden layer\n",
    "Activation functions\n",
    "Regularization techniques (e.g., L1 or L2 regularization)\n",
    "Mini-batch size\n",
    "Number of epochs\n",
    "Weight initialization methods\n",
    "Optimizers (e.g., SGD, Adam)\n",
    "If the MLP overfits, you can:\n",
    "\n",
    "Decrease the number of neurons in hidden layers\n",
    "Add dropout or regularization\n",
    "Increase the training data size\n",
    "Decrease the learning rate\n",
    "Early stopping (stop training when validation loss increases)\n",
    "Use more complex architectures like convolutional layers or recurrent layers for specific tasks.\n",
    "Tuning these hyperparameters and experimenting with different combinations can help mitigate overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
