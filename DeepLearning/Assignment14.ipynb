{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f43741",
   "metadata": {},
   "outputs": [],
   "source": [
    "Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "Answer: No, it's not okay to initialize all weights to the same value, even if that value is chosen randomly using He initialization. Initializing all weights to the same value can lead to symmetry issues in neural networks. He initialization is designed to provide a good initial spread of weights to avoid this problem.\n",
    "\n",
    "Is it okay to initialize the bias terms to 0?\n",
    "\n",
    "Answer: Yes, it is generally okay to initialize bias terms to 0. Bias terms are often initialized to 0 because they allow the network to learn the appropriate bias during training based on the data. Here's an example of initializing weights using He initialization and biases to 0 \n",
    "    \n",
    "    import tensorflow as tf\n",
    "\n",
    "# Initialize weights using He initialization\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "weights = initializer(shape=(input_size, output_size))\n",
    "\n",
    "# Initialize biases to 0\n",
    "biases = tf.zeros(shape=(output_size,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fe82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name three advantages of the ELU activation function over ReLU.\n",
    "\n",
    "Answer:\n",
    "\n",
    "ELU can handle the vanishing gradient problem better than ReLU.\n",
    "ELU has a smooth non-linearity for both positive and negative inputs, which can speed up convergence.\n",
    "ELU can also produce negative outputs, allowing it to model negative activations more effectively.\n",
    "In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "Answer: Activation function choice depends on the specific problem, but here are some general guidelines:\n",
    "\n",
    "ELU: Use ELU when you want faster convergence and better handling of vanishing gradients.\n",
    "Leaky ReLU (and variants): Leaky ReLU variants can be used when you want to mitigate the dying ReLU problem (e.g., LeakyReLU, ParametricReLU).\n",
    "ReLU: Use ReLU as a default choice for most hidden layers, as it's computationally efficient.\n",
    "tanh: Use tanh when you need activations between -1 and 1, typically in recurrent neural networks.\n",
    "logistic (sigmoid): Use sigmoid for binary classification problems in the output layer.\n",
    "softmax: Use softmax in the output layer for multi-class classification problems.\n",
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "\n",
    "Answer: Setting the momentum hyperparameter too close to 1 can lead to slow convergence and potential instability in training. The momentum term accumulates a weighted average of past gradients. If the momentum value is very close to 1, it will make the optimizer rely heavily on past gradients, causing it to oscillate around the minima or even diverge.\n",
    "\n",
    "Name three ways you can produce a sparse model.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Weight Pruning: Remove small-weight connections, setting them to zero.\n",
    "L1 Regularization (Lasso): Use L1 regularization during training, which encourages sparsity in the weights.\n",
    "Group Sparsity: Encourage entire neurons or groups of weights to be zero together.\n",
    "Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "\n",
    "Answer: Dropout can slow down training because it randomly deactivates a fraction of neurons during each training step, effectively making the network train on a subnetwork. However, during inference (making predictions on new instances), dropout is typically turned off, so it doesn't slow down the inference process. In fact, dropout can be considered a form of regularization that helps prevent overfitting during training, which can lead to better generalization and improved performance on unseen data during inference.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
