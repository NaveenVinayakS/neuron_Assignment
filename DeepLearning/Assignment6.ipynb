{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of CNN over Fully Connected DNN for Image Classification:\n",
    "\n",
    "Local Connectivity: CNNs leverage local connectivity, focusing on small regions of the input at a time. This allows them to capture local features effectively, which is crucial for image analysis.\n",
    "Parameter Sharing: CNNs use shared weights (kernels) across different parts of the image. This reduces the number of parameters, making them more efficient and capable of learning translation-invariant features.\n",
    "Hierarchical Feature Extraction: CNNs learn hierarchical features, starting with simple features like edges and gradually combining them to represent more complex patterns and objects.\n",
    "Spatial Hierarchies: CNNs maintain spatial hierarchies, which means they preserve the spatial relationships between pixels in the feature maps.\n",
    "Effective with Large Inputs: CNNs can handle large input images efficiently by using sparse connections and shared weights.\n",
    "Total Number of Parameters and RAM Usage:\n",
    "\n",
    "Each 3x3 kernel in a convolutional layer has 9 weights (parameters).\n",
    "The lowest layer has 100 feature maps, the middle layer has 200, and the top layer has 400.\n",
    "Total parameters = (100 + 200 + 400) * 9 = 6300 parameters.\n",
    "For a single instance prediction with 32-bit floats, the RAM usage for the model is approximately 24 KB (6300 parameters * 4 bytes/parameter).\n",
    "For training on a mini-batch of 50 images, you'd need approximately 1.2 MB of RAM (24 KB * 50 instances).\n",
    "Solutions for GPU Out of Memory:\n",
    "\n",
    "Reduce Batch Size: Use smaller mini-batches during training.\n",
    "Reduce Model Complexity: Decrease the number of layers, neurons, or parameters.\n",
    "Lower Resolution: Resize input images to a smaller resolution.\n",
    "Use Mixed Precision: Use lower-precision floating-point formats (e.g., float16) for model weights.\n",
    "Gradient Accumulation: Accumulate gradients over multiple mini-batches before updating weights.\n",
    "Max Pooling vs. Convolutional Layer with Same Stride:\n",
    "\n",
    "Max pooling layers downsample feature maps by selecting the maximum value in a pooling window.\n",
    "A convolutional layer with the same stride can downsample as well, but it doesn't have the invariance property that max pooling offers.\n",
    "Max pooling introduces translation invariance by selecting the maximum value, making it robust to small translations of features. A convolutional layer with stride doesn't inherently have this property.\n",
    "Local Response Normalization Layer:\n",
    "\n",
    "Local Response Normalization (LRN) layers are used to introduce lateral inhibition among neurons in the same feature map. They normalize the responses in a local neighborhood, promoting competition between neighboring neurons.\n",
    "LRN is used to enhance contrast between local features and improve the network's ability to generalize to different lighting conditions and patterns.\n",
    "It was used in some older architectures but has been largely replaced by batch normalization in modern networks.\n",
    "Innovations in AlexNet, GoogLeNet, ResNet, SENet, and Xception:\n",
    "\n",
    "AlexNet: Introduced the concept of deep convolutional neural networks for image classification and won the ImageNet competition in 2012. It featured techniques like ReLU activation, dropout, and data augmentation.\n",
    "GoogLeNet (Inception): Introduced the Inception architecture with inception modules, which allow multiple filter sizes and paths in parallel. It aims to capture features at multiple scales effectively.\n",
    "ResNet (Residual Network): Introduced residual connections, which help in training very deep networks by allowing gradients to flow directly through shortcuts. This enabled training of networks with hundreds of layers.\n",
    "SENet (Squeeze-and-Excitation Network): Introduced attention mechanisms at the channel level, allowing the network to focus on informative channels and suppress less useful ones, improving feature representation.\n",
    "Xception: Introduced depth-wise separable convolutions, which separate spatial and channel-wise convolutions. This architecture is computationally efficient and achieves high accuracy with fewer parameters.\n",
    "Fully Convolutional Network (FCN):\n",
    "\n",
    "A fully convolutional network is a neural network architecture designed for dense pixel-wise prediction tasks like semantic segmentation.\n",
    "You can convert a dense (fully connected) layer into a convolutional layer by making its kernel size equal to the spatial dimensions of the input feature map. This allows the dense layer to be applied at all spatial locations, effectively making it a convolutional operation.\n",
    "Main Difficulty of Semantic Segmentation:\n",
    "\n",
    "The main difficulty in semantic segmentation is maintaining fine-grained spatial details while learning high-level semantic information.\n",
    "It requires the network to recognize objects, understand their boundaries, and segment them pixel-wise in the input image.\n",
    "Combining local and global context information effectively without losing spatial details is challenging. Many modern architectures use skip connections, dilated convolutions, and attention mechanisms to address this issu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fe5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Use transfer learning for large image classification, going through these steps:\n",
    "a. Create a training set containing at least 100 images per class. For example, you could\n",
    "classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
    "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "b. Split it into a training set, a validation set, and a test set.\n",
    "c. Build the input pipeline, including the appropriate preprocessing operations, and\n",
    "optionally add data augmentation.\n",
    "d. Fine-tune a pretrained model on this dataset.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define data paths\n",
    "train_data_dir = r'C:\\Users\\Naveen\\Documents\\train\\cnn'\n",
    "validation_data_dir = r'C:\\Users\\Naveen\\Documents\\val\\cnn'\n",
    "test_data_dir = r'C:\\Users\\Naveen\\Documents\\test\\cnn'\n",
    "\n",
    "# Define image dimensions\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# Create data generators\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=data_augmentation,\n",
    "    validation_split=0.2  # Split a portion for validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training split\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation split\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=data_augmentation\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model (excluding top classification layer)\n",
    "base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "\n",
    "# Freeze the base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new classification head\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')  # num_classes is the number of your classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=10,  # Adjust as needed\n",
    "                    validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc504c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
