{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10547a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "What does a SavedModel contain? How do you inspect its content?\n",
    "\n",
    "A SavedModel is a serialized format for saving and restoring TensorFlow models. It contains:\n",
    "\n",
    "The model's architecture (graph).\n",
    "Trained model parameters (weights and biases).\n",
    "Signature definitions for serving and inference.\n",
    "Metadata about the model.\n",
    "\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import tensorflow as tf\n",
    "loaded_model = tf.saved_model.load(r\"C:\\Users\\Naveen\\Documents\\DeepLearning\")\n",
    "print(loaded_model.signatures)\n",
    "\n",
    "When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "\n",
    "\n",
    "Efficient model loading and serving.\n",
    "Versioning and model rollback.\n",
    "Scalability and ease of deployment.\n",
    "Support for multiple models simultaneously.\n",
    "Tools for deploying TF Serving include Docker for containerization, Kubernetes for orchestration, and TensorFlow Serving's built-in tools for model deployment.\n",
    "How do you deploy a model across multiple TF Serving instances?\n",
    "\n",
    "To deploy a model across multiple TF Serving instances, you can:\n",
    "    \n",
    "Create multiple TF Serving Docker containers or server instances.\n",
    "Load the same SavedModel into each instance.\n",
    "Use a load balancer or service discovery mechanism to distribute requests among instances.\n",
    "This setup provides high availability and load balancing for your models.\n",
    "When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "\n",
    "\n",
    "\n",
    "What are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?\n",
    "\n",
    "TensorFlow Lite (TFLite) reduces a model's size for mobile and embedded devices using techniques such as:\n",
    "Quantization: Reducing the precision of model weights (e.g., from float32 to int8).\n",
    "Weight pruning: Removing insignificant weights from the model.\n",
    "Model quantization: Combining layers and operations to minimize computation.\n",
    "Operator fusion: Merging multiple operations into a single operation.\n",
    "Post-training optimization: Applying optimizations specific to inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
