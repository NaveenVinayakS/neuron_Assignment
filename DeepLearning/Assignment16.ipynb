{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f134f7b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (4054135857.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    a) Sigmoid:\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "Explain the Activation Functions:\n",
    "a) Sigmoid:\n",
    "\n",
    "Sigmoid is a smooth, S-shaped function.\n",
    "It squashes input values to the range (0, 1).\n",
    "It's commonly used in binary classification problems.\n",
    "\n",
    "b) Tanh:\n",
    "\n",
    "Tanh is similar to sigmoid but squashes input values to the range (-1, 1).\n",
    "It's zero-centered, making optimization easier.\n",
    "\n",
    "c) ReLU (Rectified Linear Unit):\n",
    "\n",
    "ReLU is a piecewise linear function, returning x for positive values and 0 for negatives.\n",
    "It's computationally efficient and helps with the vanishing gradient problem.\n",
    "\n",
    "d) ELU (Exponential Linear Unit):\n",
    "\n",
    "ELU is similar to ReLU but smooth for negative values.\n",
    "It helps mitigate the \"dying ReLU\" problem.\n",
    "\n",
    "e) LeakyReLU:\n",
    "\n",
    "LeakyReLU is a variant of ReLU that allows a small gradient for negative values.\n",
    "\n",
    "f) Swish:\n",
    "\n",
    "Swish is a smooth, non-monotonic function that is similar to sigmoid and ReLU\n",
    "\n",
    "Optimizer Learning Rate:\n",
    "\n",
    "Increasing learning rate may lead to faster convergence but could result in overshooting the minimum.\n",
    "Decreasing learning rate can improve convergence but might make training slower.\n",
    "\n",
    "Number of Internal Hidden Neurons:\n",
    "\n",
    "Increasing hidden neurons can increase model capacity.\n",
    "It may lead to overfitting if not regularized properly.\n",
    "Batch Size:\n",
    "\n",
    "Increasing batch size can speed up training but requires more memory.\n",
    "Smaller batches introduce more noise but may lead to better generalization.\n",
    "\n",
    "Regularization to Avoid Overfitting:\n",
    "\n",
    "Regularization techniques like L1, L2, or dropout help prevent overfitting by reducing model complexity.\n",
    "\n",
    "Loss and Cost Functions:\n",
    "\n",
    "Loss functions measure the error between predicted and actual values.\n",
    "Cost functions are the average of loss functions over a dataset.\n",
    "\n",
    "Underfitting in Neural Networks:\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "It results in poor performance on both training and validation sets.\n",
    "Dropout in Neural Networks:\n",
    "\n",
    "Dropout is a regularization technique.\n",
    "It randomly sets a fraction of input units to 0 during each forward and backward pass.\n",
    "It helps prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f248954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
